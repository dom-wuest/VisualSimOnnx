{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32cb277",
   "metadata": {},
   "source": [
    "# Export CosPlace and EigenPlace Models as ONNX with Dynamic Axes\n",
    "\n",
    "The models `CosPlace` and `EigenPlace` are exported to ONNX format with dynamic axes for batch size and image size. This allows the models to handle variable input sizes during inference.\n",
    "\n",
    "The repositories for these models can be found at:\n",
    "- [CosPlace](https://github.com/gmberton/CosPlace/tree/main)\n",
    "- [EigenPlace](https://github.com/gmberton/EigenPlaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4442d",
   "metadata": {},
   "source": [
    "## Imports and Helpers\n",
    "\n",
    "These functions are used to load and export the models in ONNX format with dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def add_to_path(folder):\n",
    "    \"\"\"Add EigenPlaces/CosPlace directory to Python path.\"\"\"\n",
    "    cosplace_path = os.path.abspath(folder)\n",
    "    if cosplace_path not in sys.path:\n",
    "        sys.path.insert(0, cosplace_path)\n",
    "\n",
    "def load_image(image_path, input_size=(224, 224), mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, input_size)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - mean) / std\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC to CHW\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dim\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "def infer_onnx(model_path, img, use_gpu=True):\n",
    "    \"\"\"Run inference using ONNX model.\"\"\"\n",
    "    # --- Run ONNX inference ---\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu else ['CPUExecutionProvider']\n",
    "    session = ort.InferenceSession(model_path, providers=providers)\n",
    "\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    output = session.run([output_name], {input_name: img})[0]\n",
    "\n",
    "    # --- Post-process ---\n",
    "    output = np.array(output)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    return output\n",
    "\n",
    "def infer_torch(model, img):\n",
    "    \"\"\"Run inference using PyTorch model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        torch_input = torch.from_numpy(img).float()\n",
    "        torch_output = model(torch_input).cpu().numpy()\n",
    "    print(\"Output shape:\", torch_output.shape)\n",
    "    return torch_output\n",
    "\n",
    "def print_top5(output):\n",
    "    \"\"\"Print top-5 predictions.\"\"\"\n",
    "    top5_indices = np.argsort(output[0])[::-1][:5]\n",
    "    top5_scores = output[0][top5_indices]\n",
    "\n",
    "    print(\"Top-5 scores:\")\n",
    "    for i, (idx, score) in enumerate(zip(top5_indices, top5_scores)):\n",
    "        print(f\"{i+1}: Class {idx} â€” Score: {score:.4f}\")\n",
    "\n",
    "def compare_outputs(torch_output, onnx_output):\n",
    "    \"\"\"Compare outputs from PyTorch and ONNX models.\"\"\"\n",
    "    diff = np.abs(torch_output - onnx_output).max()\n",
    "    print(\"Max difference between PyTorch and ONNX outputs:\", diff)\n",
    "\n",
    "torch.version.__version__  # Ensure PyTorch is installed and check version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afed8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"img.png\"  # Path to your test image\n",
    "img = load_image(image_path, input_size=(1024,1024))  # Load and preprocess the image\n",
    "img.shape  # Check the shape of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c5b4",
   "metadata": {},
   "source": [
    "### EigenPlaces\n",
    "\n",
    "We start by loading a pretrained eigenplaces model and running inference on a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = torch.hub.load(\"gmberton/eigenplaces\", \"get_trained_model\", backbone=\"ResNet18\", fc_output_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_original = infer_torch(model_original, img)  # Run inference with PyTorch model\n",
    "print_top5(output_original)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e17ed",
   "metadata": {},
   "source": [
    "Next, we load the modified layers and reload the model with the new layers. This is necessary to ensure that the model can export dynamic input sizes. \n",
    "\n",
    "**IMPORTANT**: Restart the kernel at this point to ensure the model is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_path(\"EigenPlaces\")  # Add EigenPlaces directory to path\n",
    "from hubconf import get_trained_model\n",
    "\n",
    "model = get_trained_model(backbone=\"ResNet18\", fc_output_dim=512)  # Load modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59097f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_modified = infer_torch(model, img)  # Run inference with PyTorch model\n",
    "print_top5(output_modified)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c76bb3",
   "metadata": {},
   "source": [
    "Finally, we can export the modified model to ONNX format with dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"ImageEmbedding_EigenPlace_ResNet18_512.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab07f5",
   "metadata": {},
   "source": [
    "After we export the model, we can run inference on the ONNX model to verify that it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_onnx = infer_onnx(\"ImageEmbedding_EigenPlace_ResNet18_512.onnx\", img)  # Run inference on ONNX model\n",
    "print_top5(output_onnx)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39bab9",
   "metadata": {},
   "source": [
    "While we're at it we can also export a fixed sized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"ImageEmbedding_EigenPlace_ResNet18_512_512x512.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87be7b",
   "metadata": {},
   "source": [
    "## CosPlace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a314841",
   "metadata": {},
   "source": [
    "As above, load the official CosPlace model and run inference on a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56808ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = torch.hub.load(\"gmberton/cosplace\", \"get_trained_model\", backbone=\"ResNet18\", fc_output_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_original = infer_torch(model_original, img)  # Run inference with PyTorch model\n",
    "print_top5(output_original)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27830c95",
   "metadata": {},
   "source": [
    "Next, load the modified layers. Again, this is necessary to ensure that the model can export dynamic input sizes.\n",
    "\n",
    "**IMPORTANT** You must restart the kernel at this point to ensure the model is loaded correctly.\n",
    "\n",
    "Finally, we can export the modified model to ONNX format with dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e96000",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_path(\"CosPlace\")  # Add CosPlace directory to path\n",
    "from hubconf import get_trained_model\n",
    "\n",
    "model = get_trained_model(backbone=\"ResNet18\", fc_output_dim=32)  # Load modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_modified = infer_torch(model, img)  # Run inference with PyTorch model\n",
    "print_top5(output_modified)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7259a9",
   "metadata": {},
   "source": [
    "Finally, we can export the model to ONNX format with dynamic axes for batch size and image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"ImageEmbedding_CosPlace_ResNet18_32.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba23be",
   "metadata": {},
   "source": [
    "After exporting, we can run inference on the ONNX model to verify that it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_onnx = infer_onnx(\"ImageEmbedding_CosPlace_ResNet18_32.onnx\", img)  # Run inference on ONNX model\n",
    "print_top5(output_onnx)  # Print top-5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefab256",
   "metadata": {},
   "source": [
    "While we're at it we can also export a fixed sized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32115dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"ImageEmbedding_CosPlace_ResNet18_32_512x512.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-onnx-legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
